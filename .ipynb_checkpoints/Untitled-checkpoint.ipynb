{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "import time\n",
    "import importlib\n",
    "import argparse\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from network.net import net_option\n",
    "from utils.logger import MyLog\n",
    "from utils.core_utils import count_parameters\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Global Parameter\n",
    "######################################\n",
    "softmax = torch.nn.Softmax(dim = 1)\n",
    "SHOW_SEG_RESULT = True\n",
    "confidence_threshold = 0.7\n",
    "DEBUG_MIN_OBJ_WIDTH = True\n",
    "wmin = 100\n",
    "\n",
    "pick_interval = 8\n",
    "min_distance_threshold = 50\n",
    "min_angle_threshold = 60\n",
    "\n",
    "LINE_WIDTH = 2\n",
    "\n",
    "color_polate_4cls = {1: \"#00FF00\",\n",
    "                     2: \"#0000FF\",\n",
    "                     3: \"#FFFF00\",\n",
    "                     4: \"#00FFFF\",\n",
    "                     5: \"#FF0000\"}\n",
    "\n",
    "color_polate_4cls_QT = {3: \"FF0\",\n",
    "                        4: \"0FF\",\n",
    "                        5: \"F00\"}\n",
    "\n",
    "\n",
    "cmap_4cls = {1: (  0,255,  0),\n",
    "             2: (  0,  0,255),\n",
    "             3: (255,255,  0),\n",
    "             4: (  0,255,255),\n",
    "             5: (255,  0,  0)}\n",
    "alpha = 0.5\n",
    "\n",
    "\n",
    "INPUT_SHAPE = (256,512 ,3) #row col\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "######################################\n",
    "# Global Parameter\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arguments():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"-m\", dest = \"model\", type = str, help = \"Network archtecture.\")\n",
    "    parser.add_argument(\"-i\", dest = \"input_size\", type = int, nargs = \"+\", default = [256,512], help = \"Network input size.\")\n",
    "    parser.add_argument(\"-c\", dest = \"checkpoint\", type = str, help = \"Checkpoint file.\")\n",
    "    parser.add_argument(\"-iv\", dest = \"input_video\", type = str, required = True, help = \"Input video for demo.\")\n",
    "    parser.add_argument(\"-ot\", dest = \"od_threshold\",  type = float, default = 0.3, help = \"Detection Confidence threshold.\")\n",
    "    parser.add_argument(\"-ov\", dest = \"output_video\", action = \"store_true\", help = \"Input video for demo.\")\n",
    "    parser.add_argument(\"-camera\", dest = \"camera\", action = \"store_true\", help = \"Input video for demo.\")\n",
    "    return parser.parse_args()\n",
    "# args = get_arguments()\n",
    "\n",
    "model_type = 'Jacinto_256x512_v3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "# Global Parameter\n",
    "######################################\n",
    "cap = cv2.VideoCapture(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('num_classes_OD', 3)\n",
      "('num_classes_seg', 8)\n",
      "('use_focal_loss', False)\n",
      "('self.pyramid_levels', [3, 4, 4, 5, 5, 6, 6, 7])\n",
      "('self.min_sizes', [20.48, 25.6, 25.6, 51.2, 51.2, 76.3, 76.3, 128.0])\n",
      "('self.max_sizes', [51.2, 51.2, 51.2, 76.8, 76.8, 128.0, 128.0, 176.17])\n",
      "('self.offsets', [0.5, 0.5, 1.0, 0.5, 1.0, 0.5, 1.0, 0.5])\n",
      "('self.normalize_anchor', False)\n"
     ]
    }
   ],
   "source": [
    "model_check_point = 'weights/sur_object_detection/Jacinto_ssd_256x512_256x512_detection_v2_sur4_bs_16_lr_1e-05_fixbackbone_False_freeze_bn_False_sampler_False_normalize_coor_False_Jacinto_SSD_neg_pos6_499.pt'\n",
    "\n",
    "net = net_option(model_type, mode = \"end2end\")\n",
    "net = net.to(device)\n",
    "\n",
    "# resume from checkpoint\n",
    "assert os.path.exists(model_check_point), \"Checkpoint {} does not exist.\".format(model_check_point)\n",
    "state = torch.load(model_check_point)\n",
    "net.load_state_dict(state[\"model_state\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------\n",
      "Preprocess time: 0.001\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "need more than 3 values to unpack",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-7a29cbc917a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;31m# net forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformed_anchors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_seg_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mpred_seg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_seg_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mtoc\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: need more than 3 values to unpack"
     ]
    }
   ],
   "source": [
    "######################################\n",
    "# Main Process\n",
    "######################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "skip_frame = 1\n",
    "count = 0\n",
    "net.target_available = False\n",
    "net.eval()\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    while(cap.isOpened()):\n",
    "        ret,frame = cap.read()\n",
    "        assert frame.shape[0]>0\n",
    "        \n",
    "        # skip until spicific frame\n",
    "        if(count%skip_frame!=0):\n",
    "            count += 1\n",
    "            continue\n",
    "        else:\n",
    "            count +=1\n",
    "        \n",
    "        start = time.time()\n",
    "        RGB = np.zeros(INPUT_SHAPE,dtype=np.uint8)\n",
    "        print(\"-------------------\")\n",
    "        tic = time.time()\n",
    "        \n",
    "        #resize image\n",
    "        img = cv2.resize(frame, (INPUT_SHAPE[1], INPUT_SHAPE[0]), 0, 0, interpolation = cv2.INTER_LINEAR)\n",
    "        #BRG to RGB\n",
    "        img = img[:,:,[2,1,0]]\n",
    "        # transfrom and expand dim\n",
    "        input = img.transpose((2, 0, 1))\n",
    "        input = np.expand_dims(input, axis = 0)\n",
    "        # push to tensor\n",
    "        input = torch.from_numpy(input).float()\n",
    "        input = input.to(device)\n",
    "        toc = time.time()\n",
    "        print(\"Preprocess time: {:.3f}\".format(toc - tic))\n",
    "        # net forward\n",
    "        tic =time.time()\n",
    "        scores, classification, transformed_anchors, pred_seg_t = net(input)\n",
    "        pred_seg = softmax(pred_seg_t)\n",
    "        toc =time.time()\n",
    "        print(\"Net forward time: {}\".format(toc - tic))\n",
    "        tic =time.time()\n",
    "        \n",
    "        pred_seg = pred_seg.squeeze(dim = 0)\n",
    "        pred_seg = pred_seg.data.cpu().numpy()\n",
    "        pred_seg[pred_seg < confidence_threshold] = 0\n",
    "        img_draw = img.astype(np.uint8)\n",
    "        prob_map = np.max(pred_seg[3:, :, :], axis = 0)\n",
    "        pred_seg_max = np.argmax(pred_seg, axis = 0)\n",
    "        toc =time.time()\n",
    "        print(\"Pre-Post-process time: {}\".format(toc - tic))\n",
    "        \n",
    "        \n",
    "        tic = time.time()\n",
    "        overlay_flag = np.zeros((INPUT_SHAPE[0], INPUT_SHAPE[1]))\n",
    "        # draw\n",
    "        for key, color in cmap_4cls.items():\n",
    "        # for key, color in dataset.color_map.items():\n",
    "            if key == 0:\n",
    "                continue\n",
    "            if key >= 3:\n",
    "                continue\n",
    "            else:\n",
    "                RGB[pred_seg_max_up == key] = np.array(color)\n",
    "                overlay_flag[pred_seg_max_up == key] = 1\n",
    "        # overlay\n",
    "        overlay = cv2.addWeighted(img_draw, alpha, RGB, (1-alpha), 0)\n",
    "        # overlay2 = cv2.addWeighted(img_draw2, alpha, RGB2, (1-alpha), 0)\n",
    "        img_draw[overlay_flag == 1] = overlay[overlay_flag == 1]\n",
    "        img_draw_down = img_draw.copy()\n",
    "        \n",
    "        toc =time.time()\n",
    "        print(\"Segmentation draw time: {}\".format(toc - tic))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NMS took 0.0349678993225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[tensor([0.2526, 0.2567, 0.2548,  ..., 0.2568, 0.2568, 0.2554], device='cuda:0',\n",
       "        grad_fn=<MaxBackward0>),\n",
       " tensor([0, 1, 1,  ..., 1, 1, 0], device='cuda:0'),\n",
       " tensor([[  0.0000,   0.0000,  21.7824,   9.8759],\n",
       "         [  0.0000,   0.0000,   9.9588,  21.6731],\n",
       "         [  6.0516,   0.0000,  17.9619,  21.6628],\n",
       "         ...,\n",
       "         [101.4241, 146.9913, 282.1036, 237.2735],\n",
       "         [229.4254, 146.9911, 410.1025, 237.2723],\n",
       "         [384.1982, 128.0954, 512.0000, 255.6854]], device='cuda:0',\n",
       "        grad_fn=<IndexBackward>)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
